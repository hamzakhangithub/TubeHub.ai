{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbyX5sf0gnF7",
        "outputId": "6f6d780f-7e76-4bfe-f278-f85ed9ff5478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "    langchain==0.0.292 \\\n",
        "    openai==0.28.0 \\\n",
        "    datasets==2.10.1 \\\n",
        "    pinecone-client==2.2.4 \\\n",
        "    tiktoken==0.5.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deeplake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x8jNasngzvG",
        "outputId": "d0252d94-129f-46ea-ce57-7f2e2f73cb61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deeplake\n",
            "  Downloading deeplake-3.8.15.tar.gz (588 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/588.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/588.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m460.8/588.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.4/588.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from deeplake) (9.4.0)\n",
            "Collecting boto3 (from deeplake)\n",
            "  Downloading boto3-1.34.22-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake) (8.1.7)\n",
            "Collecting pathos (from deeplake)\n",
            "  Downloading pathos-0.3.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humbug>=0.3.1 (from deeplake)\n",
            "  Downloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deeplake) (4.66.1)\n",
            "Collecting lz4 (from deeplake)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from deeplake) (2.3.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.10.13)\n",
            "Collecting libdeeplake==0.0.98 (from deeplake)\n",
            "  Downloading libdeeplake-0.0.98-cp310-cp310-manylinux2014_x86_64.whl (16.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aioboto3>=10.4.0 (from deeplake)\n",
            "  Downloading aioboto3-12.2.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.5.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from libdeeplake==0.0.98->deeplake) (0.3.6)\n",
            "Collecting aiobotocore[boto3]==2.9.0 (from aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aiobotocore-2.9.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.33.14,>=1.33.2 (from aiobotocore[boto3]==2.9.0->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading botocore-1.33.13-py3-none-any.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.7.4.post0 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.9.0->aioboto3>=10.4.0->deeplake) (3.9.1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.9.0->aioboto3>=10.4.0->deeplake) (1.14.1)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore[boto3]==2.9.0->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting boto3 (from deeplake)\n",
            "  Downloading boto3-1.33.13-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->deeplake)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.9.0,>=0.8.2 (from boto3->deeplake)\n",
            "  Downloading s3transfer-0.8.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from humbug>=0.3.1->deeplake) (2.31.0)\n",
            "Collecting ppft>=1.7.6.7 (from pathos->deeplake)\n",
            "  Downloading ppft-1.7.6.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from libdeeplake==0.0.98->deeplake)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.3 (from pathos->deeplake)\n",
            "  Downloading pox-0.3.3-py3-none-any.whl (29 kB)\n",
            "Collecting multiprocess>=0.70.15 (from pathos->deeplake)\n",
            "  Using cached multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deeplake) (4.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.33.14,>=1.33.2->aiobotocore[boto3]==2.9.0->aioboto3>=10.4.0->deeplake) (2.8.2)\n",
            "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.33.14,>=1.33.2->aiobotocore[boto3]==2.9.0->aioboto3>=10.4.0->deeplake) (2.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->humbug>=0.3.1->deeplake) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->humbug>=0.3.1->deeplake) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->humbug>=0.3.1->deeplake) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.9.0->aioboto3>=10.4.0->deeplake) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.9.0->aioboto3>=10.4.0->deeplake) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.9.0->aioboto3>=10.4.0->deeplake) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.9.0->aioboto3>=10.4.0->deeplake) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.9.0->aioboto3>=10.4.0->deeplake) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.9.0->aioboto3>=10.4.0->deeplake) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.33.14,>=1.33.2->aiobotocore[boto3]==2.9.0->aioboto3>=10.4.0->deeplake) (1.16.0)\n",
            "Building wheels for collected packages: deeplake\n",
            "  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplake: filename=deeplake-3.8.15-py3-none-any.whl size=708613 sha256=78708c3e33280c1b60494244101bf4815d05e4a7363658359a12bd95f531643a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/96/d3/dc538ac05fbfeb9303f25925f0dd9581cbacb0c336b10c4726\n",
            "Successfully built deeplake\n",
            "Installing collected packages: ppft, pox, lz4, jmespath, dill, aioitertools, multiprocess, libdeeplake, humbug, botocore, s3transfer, pathos, aiobotocore, boto3, aioboto3, deeplake\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.14\n",
            "    Uninstalling multiprocess-0.70.14:\n",
            "      Successfully uninstalled multiprocess-0.70.14\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 2.10.1 requires dill<0.3.7,>=0.3.0, but you have dill 0.3.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aioboto3-12.2.0 aiobotocore-2.9.0 aioitertools-0.11.0 boto3-1.33.13 botocore-1.33.13 deeplake-3.8.15 dill-0.3.7 humbug-0.3.2 jmespath-1.0.1 libdeeplake-0.0.98 lz4-4.3.3 multiprocess-0.70.15 pathos-0.3.1 pox-0.3.3 ppft-1.7.6.7 s3transfer-0.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q yt_dlp\n",
        "!pip install -q git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGojKHwfgzyp",
        "outputId": "2c6f5a42-8ffd-4012-b1c4-2e2068fddf8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "os.environ[\"ACTIVELOOP_TOKEN\"] = \"\""
      ],
      "metadata": {
        "id": "fYLL0H1sg94T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yt_dlp\n",
        "\n",
        "def download_from_youtube(url):\n",
        "  filename = 'video.mp4'\n",
        "  ydl_opts = {\n",
        "      'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
        "      'outtmpl': filename,\n",
        "      'quiet': True,\n",
        "  }\n",
        "\n",
        "  with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    result = ydl.extract_info(url, download=True)"
      ],
      "metadata": {
        "id": "2DeWFkbdg972"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.youtube.com/watch?v=N7RU6W4hAMI\"\n",
        "download_from_youtube(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s5TDurkhyms",
        "outputId": "4f7f2c9e-9667-453f-cac2-bbd0ea61c247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "model = whisper.load_model('base')\n",
        "result = model.transcribe('video.mp4')\n",
        "print(result['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZeJE20Uiv5Y",
        "outputId": "9813a252-c73e-477a-afb7-6c0c8ee8941b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:04<00:00, 32.0MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello all my name is Krish Nakhayak and welcome to my YouTube channel. So guys in this video we are going to see the perfect road map to learn data science in 2024. At the end of every year probably I upload this kind of video so that your learning are always updated with respect to data science and similar kind of videos also I upload with respect to data analyst, big data and many more. So in the next year, in January I will also be coming up with a road map to become a data analyst in 2024. But again going forward, understand this specific road map is quite amazing. The reason is that I have been uploading videos from past 5 years, I have been teaching from past 7 to 8 years, I have seen amazing career transition, I have seen so many different tools that are probably coming up in the market, I have seen what kind of skillsets are specifically required in interviews what they are asking, I have done a lot of podcasts with respect to transition stories and many more things. So considering all those things I have created this amazing road map and this road map has all the handwritten notes, it has all the projects, it has all the videos in videos format, everything you will be able to access it and trust me the videos that I am probably going to provide you and that is uploaded in the form of playlist has helped people to successfully crack jobs with respect to data science. So definitely this is my overall effort of the past 4 to 5 years. And this year in 2023, I was quite interested in creating a lot of end-to-end projects with MLOPS and Gen.A.D.V.I is one of the things now. If you go ahead and see my playlist, I have created around 10 to 15 projects on Gen.A.D.V.I using Lang Chene, using Open AI, using Google Gemini and many more. And that in 2024, I am going to continue that, I am going to focus on two different things, one is Gen.A.D.V.I and one is MLOPS. So let's go ahead and without waste of any time, let's go ahead and see this. I will keep the like target of this particular video to 2000, please make sure that you quickly do that, hit like, put some kind of comments and share with all your friends because this entire road map has all the free content that you specifically require. Everything is available over here. So let's go ahead and let's see the road map. So first thing, work of a data scientist. As you all know, the life cycle of a data science project has been almost same from past four to five years. But as we go ahead with respect to step-by-step, there are some amazing tools that have actually come. You know, I'll be talking about all those tools. It's a very big road map altogether. See, the reason it has become big because I've given multiple links, multiple video types, multiple documentation type. If you're not able to understand from documentation, I've given you videos. So if you probably scroll, they are around so many different different projects like this project will be sufficient. I've created more than 50 plus project, but I've selected somewhere around 10 projects, which you can probably use it and do any kind of projects. And there in that specific project, I've added MLOPS tools, I've had multiple things. Trust me guys, in no other YouTube channel, you will be able to find it. I'll give you that guarantee that at least just many number of projects and two projects with deployments will not be available. So to go ahead with, you know, we'll first of all understand. Initially, when we have a data science project, we go to the next step, which is called as Requirement Gathering. In the requirement gathering, domain experts and product owner, along with business analyst, they discuss a lot of things and they create a specific, you know, they create a lot of stories and at each iteration or at each sprint, how many stories needs to become completed, based on that they'll be deciding the program manager will be deciding the team size. Then all this requirement will be sent to the data analyst or data science team. Then again, the data science or data analyst team will be discussing with the product owner because we need to understand what is the data scientist do as usual. Along with this product owner and expertise, they have a lot of discussion and then they try to find out for this specific use case where they have to probably take the data from. So they will be dependent on internal databases. They'll be independent on third party cloud APIs. They may be dependent on some third party paid APIs itself right for the data itself. And once they identified, then by creating pipeline with the help of big data engineering team, all this data is efficiently stored in our specific databases. It can be MongoDB, it can be different different databases, it can be Hadoop databases, it can be, it depends on the use case and scenarios that you're specifically using. Now, once this big data engineers, they have created that pipeline at the end of the day, all those data will be now sent to the data science team. And that is where the life cycle of a data science project begins, right. The first step is feature engineering, second step is feature selection, third step is model creation hyper pamper tuning, the first step is model deployment and here a lot of ML ops is used, right. And that is the reason why I'm telling you ML ops, ML ops, this year next year, probably see because of opening AI, because of this LLM models, now the most important thing is that how you can create and project efficiently with the help of ML ops tools, right. So here, Dockerization, CICD pipeline, all those concepts and then we finally do the deployment. And finally, at the end of the day, we also do model monitoring, right. And we also look for retraining opportunities. Now in feature engineering, if you want to know more, know more about it, we specifically do exploded data analysis, handling missing values, handling outliers, categorical encoding, normalization, standardization, correlation, forward animation, backward animation, univariate selection, random forest importance and all and all are there, right. See guys, if you go, probably go ahead, right. This roadmap looks quite but understand I've created multiple players, both in English and Hindi language, right. So if you are interested in learning in Hindi, you can go ahead with, you can if you are interested in learning in English, you can go ahead. There are still many videos that I really need to upload in Hindi also. So that will be my target also in 2024. Okay. So first thing first, what is the prerequisite that is probably required or what is the first skill set that is actually required to learn data science. One is programming language and again, I would always suggest from past three years I'm suggesting, go ahead with Python programming language because Python programming language is improving extensively, extensively means there's so many libraries, there's so many things that are coming up, any companies that are even coming up with the LLM models or any feature, first of all, they make sure that they are, it is compatible to Python because the communities use many companies are using it, many companies are supporting for this specific development and Python is very easy to use. Okay. So again, in this time, when charge-ypities there, when videos formats are, there's so many things that I've already created a detailed playlist both in English and Hindi, you can probably see over here, right. So this is basically in English and this is in Hindi, right. I've created a detailed playlist, you can definitely follow this and you can learn Python within one month. Why I'm saying one month? If you are devoting two to three months every day, sorry, two to three hours every day, then trust me to be very much easy and again to see more and more examples, you have to use chat GPT, okay. Try to use chat GPT or Googlebot, try to ask more different examples over there. Many people say that, Krish, we don't know like what kind of examples will be coming up, how to practice. Go and ask chat GPT, it will give you hundreds of examples. Now your learning process has become very simple, right. All you need to be, is becoming smart and practicing more and more with the right kind of questions over there, right. So with respect to this Python, I've actually created three playlists, one is Python playlist in English and Hindi. And to support this, I've also created a flask framework playlist, okay. What is the final goal of outcome with respect to Python? Here you can see basic to intermediate Python with various knowledge of various data structures, like NumPy, Pandas, Matplot, Lim and many more. Knowledge of performing ADF feature engineering and creating visualization charts using Python, at least makes some Python projects using frameworks such as Plask with deployment. When I say end to end Python project, I'm talking about deployments, I'm talking about this. So that this becomes very much easy when you go to the next step. Again, at the end of the day, you want to become a data scientist, right. So that is the reason why I'm telling you this, okay. Then the next thing is statistics. Statistics as usual is quite amazing, okay. Statistics is the most important thing in data science, whether you're working in machine learning, whether you're probably working in LP, anything where you work in, right. Statistics can play a very important role. And for this also, I've created some amazing sessions, there are three amazing playlist even in English. Hindi, everything is there, right. They are around 43 videos in English. They are 20 to 22 videos in Hindi. And still, I need to complete more, more videos I'll try to upload as we go ahead. But this is the fundamental thing that is basically required. Statistically, because of statistics, you'll be able to understand how you can use this amazing tool. See, maths, machine learning is what? Machine learning actually provides you stats tools to analyze the data, to visualize the data, to do future prediction, to probably create model and many more things. So statistics can be very much amazing. See, and all this are created with amazing links. All the materials will be available and just imagine you just need to learn, right. And I'll give you a technique like what you should definitely do with respect to learning. So here you can probably see English video, live session of statistics have created seven days live session, which completes the entire statistics you can learn in this way. And Hindi also have created around 20 videos. That also you can probably find out. And everything is present over here. You can follow any one of them according to your comfortability, right. First we started with Python, then probably complete statistics, then once you do that, learn about feature engineering, right. Feature engineering. And this is for all the people who want to learn from basic, okay. Who wants to learn from basic? It's not like I'm telling you that you always need to start it from here. If you know all these things and I think people who are following me in my YouTube channel, they should be knowing at least this much because this is what I've been uploading from past three to four years, right. This year I've focused more on end-to-end projects, right. So here is a feature engineering both in live playlist and complete detailed idea. So you can also watch this. The final outcome will be that techniques to perform statistical analysis. More I'll be talking about internship, many more things, jobs, everything I'll be talking about as we go ahead. Then coming to the next one is about databases. In databases I usually say focus on one SQL and one most equal databases. For no SQL I've selected MongoDB. You can also use Cassandra. You can also use other MongoDBs. All these no SQL databases will now play a very important role because now in generative AI there is something called as vector embedding. And most of the time no SQL databases is used and all those vectors. Vectors basically means what? Whenever text is basically converted into when the word is converted into some numerical format, right. So that the machine learning algorithms will be able to understand it, right. So that is reason my again, one focus will be on no SQL databases and again I've created a playlist on this. My SQL databases and plan is that that in the future I will try to create the Cassandra database which is a passicker, Chandra. The documentation link is also given over here. So everything I will try to upload this. So please make sure that guys you for this repository star this particular repository so that it will be handy for you. Okay. Then coming to the next one machine learning as usual guys there is a there is a thing right there's a some questions now generative AI is coming up. Can we learn generative AI without machine learning or deep learning the answer is yes you can you can directly go and job. Okay. No worries. But it is always good if you have your fundamental strong and that fundamentals why I love about data science from 2014 I'm working on it. You can just imagine the kind of fundamental base that I have actually built. If you go ahead and see my every video where I've used mathematical intuition where I've taught multiple things my fundamental base is very much important over there, right. The concept is very good. So that even though if I even now also if I probably go ahead and give any interview without any preparation I will be definitely able to answer things because my fundamentals are strong. So if you also want to make your fundamental strong right I would always suggest start with machine learning the best way again telling you guys you can start with this live ML playlist right. So here in live ML place have in six to seven days have completed almost all the algorithms you can go ahead and see it and this is with practical implementation. Okay. This is with practical implementation. So that will be quite amazing. Then coming to the next one deep learning playlist see any of this you can probably select it is upon your thing if you want to go slow go ahead with this longer playlist. If you don't go fast I'm going to prepare quickly then if you have lot of time then probably use this second one if you have less time go ahead with live ML playlist you can see two to hours videos go ahead with that not a problem. If you want to learn in Hindi Hindi is also there all the algorithms are specifically uploaded. After you complete machine learning then we come to a subset of machine learning which is called as deep learning. Now guys many people talk about deep learning right deep learning the major thing is ANN CNN RNN the variation of RNN the variation of CNN object detection and many more things right. So for this also we have created this specific playlist. So there is a five days live playlist there is complete deep learning playlist and also there is a deep learning playlist in Hindi. So as I said my main aim is to provide you content in a way that you don't have any language barrier problem you don't have any content problem itself right. That is what is my main my main aim is to democratize this entire AI education right. So that you will be able to learn in an amazing way that is what I specifically want. So here you can probably see this three playlist you can start with and I have probably completed elements almost everything NLP I've completed a deep learning I've completed everything along with materials along with free guides along with tutorials along with videos everything trust me everything is over here and I want this repository to go into multiple places so that people also watch this because at the end of the day they get to know about things and all the playlist is structured in a better manner right. Now once you complete deep learning again in deep learning also you need to focus a lot guys you need to understand how TensorFlow works how PyTorch works you need to probably understand how things with respect to learning I'm telling you right and nowadays since generative AI LLMs right. So focus more on the NLP part if you're focusing more on large image models and focus on computer vision part so that is how you can probably categorize things okay then coming to the NLP playlist so here you have English live NLP playlist and complete NLP playlist okay so this is again there and you can also go ahead and walk in this all I did it in last year so that it is very much handy and available to it and right now also it has a lot of views people are using it and many more things okay now once you complete all these things it's at the end of the day you really need to create a lot of projects when I say projects it's just not like capstone project that capstone project that Jupyter notebook project not that project the project is something that you should develop completely end to end you should use one repository where you are committing the code you should talk about MLOps techniques you should talk about GitHub actions you should talk about multiple things you should talk about modular coding you should talk about logging you should talk databases many more things as such so for this also it is good that you know some of the important frameworks one is the flash detailed playlist that I have already created and then one is flash one shot video, Gradio, Ben to ML ML flow and DAGs up I will talk more about this particular frameworks but this is specifically required for production deployment if I talk about DVC all these things right so it is good that you know or you have some idea about this kind of frameworks so for this frameworks is pretty much important still we have a lot of many things now this is the most important thing guys this is the most important I still say this is the most most most important thing that is MLOps machine learning operations that is with respect to design model development and operation as I said guys what is going to happen in the future now if you probably see lot of LLM models or LLM models these models are having huge accuracy because those bigger companies which have bigger GPUs bigger bigger datasets they are able to train models with so much accuracy even in hugging face if you go ahead and see with respect to every machine learning deep learning object detection any model they already have some amazing pre-trained model and over there you can just go ahead and fine tune and you can use their API but understand accuracy part of the model is already solved right it has been solved and it has been solved by this bigger company is already and again for a startup to probably do that rework it will take time because at the end of the day this was the problem that was happening a startup AI startup if you see previously that were coming they you should take a lot of time to develop their product but this year as soon as generative AI became very much popular you could see that there was so many products so many startups that I actually started AI startup itself right you admit journey so many different things at the end of the day they using some of the other models from this amazing giants right so what is the most important thing after that integrating with their platform right and that is where ML ops or DevOps will come into picture so for this I have created a lot of videos guys for some of the things I have not created it but again in first of all I would suggest always focus on GitHub CI CD pipelines GitHub actions then you have circle CI you have cube you have ML flow ML flow I have already developed a video deployment techniques in AWS Azure Docker and Kubernetes I have done this in my project playlist which I will show you just down then there is something called as evidently AI so if you probably go and see evidently AI this is another amazing open source ML observatory platform where you will be able to see how your model is performing and how you can basically do so I have also created a YouTube video then you have Grafana monitoring you have air flow you have bento ML you have Sage maker I have created a detailed place and projects on Sage maker you have DVC you have Docker many more things so the reason why I put this many things is it is not necessary that you have to learn each and everything but at least learn some of the handy machine learning frameworks you know the complete machine learning lifecycle of the project which can be handled by ML flow or if you have bento ML something like that so you have to learn all these things whichever you find it easy whichever is required in the industry can go ahead with this but the best thing is that most of the frameworks have actually covered videos is also developed and you will be able to see where I have not given video that kind of tutorials has also been uploaded in the YouTube channel in some of the projects so here you can probably see end to end ML DL projects with ML observatory deployment and open source stores so here is the two playlist that I really wanted to show you if you have never developed a project how you have to probably do the modular coding how you have to probably do the logging how you have to go ahead with doing each and every steps what is the process of creating a project this we have created and many people have shared and many people have put it in their resume and trust me when you have this kind of projects explaining the interview becomes very very much easy right as I said that I have uploaded more than 50 plus end to end project videos but out of them I have selected this 10 okay so here you can see first end to end ML project for starters student profile and prediction end to end NLP project end to end machine learning project use AWS SageMica computer vision end to end cell segmentation deep learning project with deployment ML ops DVC end to end ML flow projects projects with ML flow end to end ML project implementation docker's GitHub action Langchain Open AI project kidney classification project Langchain Open AI so over here you will be able to see I have uploaded every kind of project let it be NLP let it be deep learning let it be machine learning trust me guys I don't know you will never be able to get this much along with video explanation along with materials everything the reason is that everyone I I my aim is again to democratize AI education I want everyone to learn AI not to just get job right my main aim not to get job to understand the importance of AI try to use it in your day to day activities right so all those things you can specifically do right then once you probably complete this specific project one by one project you can do yes if you know all those things that you have learnt earlier this project becomes very very much easy then coming to the detailed generative AI and LLM playlist so here you can see Open AI playlist English Langchain playlist Google Germany playlist I have also uploaded this and if you want to specifically do any open source contribution I have already created a dedicated video and created one repository which has 4.7 K star so here you can probably see in ML projects they are a lot of projects that are probably coming over here you can also go ahead and contribute it so that many people will be able to see it right or use it because and as I have told you can also do your branding over here you can put your profile link and all whatever things you specifically want to do okay that you can actually do now this is the most important thing use of chat deputy or Googlebot extensively now guys whenever I you may be thinking that how Chris you are able to upload daily video see the main thing is like let's say with respect to generative AI recently I was checking with respect to Germany I was able to see Germany pro right API I was able to see and do it in Jupyter Notebook very much quickly okay because those documentation link was already provided but again at the end of the day to convert that as an end to end project it becomes very much difficult because you need to create a front end you need to make sure you can you have to use some frameworks like stream let's ask anything as I what I did I simply go I went ahead and asked chat deputy hey provide me a template where I am going to use the stream let probably I want to front end with one button one submit button one upload button as soon as I click the submit it should hit my Germany API so that entire template was specifically created by chat deputy and was available to me right so that is the most amazing thing right so at the end of the day this is the future right here you are saving time you are becoming much more productive when you are probably using a coding this right it becomes very much easy because initially which was taking five years now it is just so I mean initially let's say work is taking five hours now it is probably taking one hour why because of chat deputy you don't have to probably go and do much Google give the right context get the right answer try to use it try to debug it and try to solve it right so this is the most important thing and this will be the most important thing that will stand out with the current data scientist and with the previous data scientist if they are using this right it is quite amazing you will definitely be productive and I don't have to create a tutorial how you can use chat deputy that is by your side itself right you can actually do it see now at the end of the day you may be thinking that you have given such a big road map for the people who are starting right now how we should go ahead see for the people who are starting right now you can go ahead with this thing only not a problem say it may take six to seven months right but the most important will be practice like there if there is a full stack developer he wants he or she wants to work with generative where you can directly jump with generative where that is there okay but my major aim is to learn in such a way that your basics is also strong your fundamentals is also strong and it is always good to build knowledge slowly when you start in the right manner I have not suggest that always learn after idiot directly down jump to deep learning I don't want that right by that specific you want to be able to understand right because if your fundamental is strong guys tomorrow you work anywhere any new technology any new things come you will be able to grasp it very much quickly right take my example right anything when you comes I just take a couple of hours I read about it I practice something and I am able to create a video about it I am able to teach you right the same thing will happen to you when your basics and fundamental are very very much strong at the end of the day after learning this much guys for a freshers I would always suggest for the people who I have a carrier gaps I would always suggest two internships right while learning try to find out internships because for a fresher many people to say that hey fresher do not have jobs in data science see data science jobs are something that companies look with respect to experience right the kind of work that you specifically do in a data science project is very much crucial it can create a lot of loss in the companies if it is not done in a proper way right so internships can actually help you to gain that specific experience even though you have a carrier gap I had so many students who are doing government exams they were they were they were they spent four to five years you know they have four to five years of carrier gap because they were preparing for government exams but just because of internships they are again able to come back to the jobs and internships trust me guys is easy to get when compared to a full-time job for freshers if you have at least six months experience then it is easy for you to get into the industry but without experience it becomes very much difficult right so that is the reason why I would always suggest go ahead and apply with internships if you keep on applying in internships you are going to get an opportunity I've seen with respect to full-time jobs for a fresher and if I compare it with internships they able to get internships very much quickly and there are a lot of messages that have already put up in the LinkedIn for experience be person who are working in different technology they can actually make a transition over here right for them whatever work they have done over there just see how they can apply data science over there and create a proof of concept project and by that way they will be able to put those things in in their resume I'm not telling you to write anything as a lie over there whatever things you have learned and how you have actually implemented your previous project have you applied any data science concept over there if you are not able to apply some or the other problem will be there at least somewhere right try to see if there is able if you are able to find it out and then probably try to convert into a POC project and then try to put that in your resume and explain the same thing in front of the interview right you have a genuine work right so once you probably do the internship internship is important again I neuron provides you internship right so there is an internship platform many many people they have more than 500 projects over here you can sign in it has it provides you the entire dashboard it provides everything as such what you have to do is that just go ahead and probably enroll for the internship complete the project submit the projects because as soon as you enroll in internship also you get an offer later and as soon as you completed submit it you will get that many number of months experience let's say if you are taking six months to complete a project whatever project is mentioned over here then that six months experience you will be able to get as an internship certificate okay so internships is super important and here I have also created a data center tracker sheet which you can use all the other things you will be able to see in all the channels so guys this was it for my side I hope you like this particular video but at the end of the day guys continuous learning is a very important in the field of data science the more you learn the more you become better so I'd always suggest have or always a mindset of learning more and more seeing new things because that will actually help you to grow in the industry so yes this was it for my side I'll see you all in the next video I have great thank you and I'll take care bye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open ('text.txt', 'w') as file:\n",
        "    file.write(result['text'])"
      ],
      "metadata": {
        "id": "b8BuYP2siv89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain import OpenAI, LLMChain\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "eFWY4vUUmbLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model='gpt-4', temperature=0)"
      ],
      "metadata": {
        "id": "2y05bg0Tm4xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
        ")"
      ],
      "metadata": {
        "id": "qX-5wLzTm__6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "with open('text.txt') as f:\n",
        "    text = f.read()\n",
        "\n",
        "texts = text_splitter.split_text(text)\n",
        "docs = [Document(page_content=t) for t in texts[:4]]"
      ],
      "metadata": {
        "id": "4FFBd0fDnADh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import textwrap\n",
        "\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "wrapped_text = textwrap.fill(output_summary, width=100)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJfyhb0fnALB",
        "outputId": "b44058a8-427d-4602-ad00-39b93eda2193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data science educator, Krish Nakhayak, has introduced a YouTube channel dedicated to data science,\n",
            "data analysis, and big data learning resources. He plans to release a roadmap for learning data\n",
            "science and becoming a data analyst in 2024, based on his extensive teaching experience and industry\n",
            "knowledge. The roadmap, which includes handwritten notes, projects, and videos, is designed to help\n",
            "individuals secure jobs in data science. The channel also features 10 to 15 projects focused on\n",
            "MLOPS and Gen.A.D.V.I. The video aims to provide a comprehensive, free content roadmap for data\n",
            "science, including the lifecycle of a data science project and various tools and resources.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tmkhGu64nalM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AqfscxxYnaow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Write a concise bullet point summary of the following:\n",
        "\n",
        "\n",
        "{text}\n",
        "\n",
        "\n",
        "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
        "\n",
        "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template,\n",
        "                        input_variables=[\"text\"])"
      ],
      "metadata": {
        "id": "FPPW4o06l5jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"stuff\",\n",
        "                             prompt=BULLET_POINT_PROMPT)\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "\n",
        "wrapped_text = textwrap.fill(output_summary,\n",
        "                             width=1000,\n",
        "                             break_long_words=False,\n",
        "                             replace_whitespace=False)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud23WHwUl51d",
        "outputId": "61170be1-15d5-4c05-91ee-a04aa7b8b7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Krish Nakhayak introduces his YouTube channel and announces a video on the perfect roadmap to learn data science in 2024.\n",
            "- He regularly updates his viewers with videos on data science, data analysis, big data, and more.\n",
            "- Krish has been teaching for 7-8 years and has seen many career transitions and new tools emerging in the market.\n",
            "- He has created a roadmap with handwritten notes, projects, and videos, which has helped many people secure jobs in data science.\n",
            "- In 2023, he focused on creating end-to-end projects with MLOPS and Gen.A.D.V.I, and plans to continue this focus in 2024.\n",
            "- The roadmap includes free content and he encourages viewers to like, comment, and share the video.\n",
            "- He explains the life cycle of a data science project and introduces new tools that have emerged.\n",
            "- The roadmap includes links, video types, and documentation for better understanding.\n",
            "- He has selected around 10 projects from over 50 he has created, which include MLOPS tools and other elements.\n",
            "- He\n",
            "guarantees that his channel offers a unique number of projects and deployments not available elsewhere.\n",
            "- He explains the initial steps of a data science project, including requirement gathering involving domain experts, product owners, and business analysts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "wrapped_text = textwrap.fill(output_summary, width=100)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiAgzSI6n16Z",
        "outputId": "2dda113d-9f0b-471b-a170-c8acf2cad7ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Krish Nakhayak, a seasoned YouTube content creator and data science educator, is set to release a\n",
            "comprehensive video roadmap for learning data science in 2024. This roadmap, which will be updated\n",
            "annually, includes handwritten notes, projects, and video tutorials that have proven successful in\n",
            "helping individuals secure jobs in the data science field. Nakhayak's insights are drawn from his\n",
            "years of experience, industry trends, and interviews. In 2023, he focused on creating end-to-end\n",
            "projects with MLOPS and Gen.A.D.V.I, producing 10 to 15 projects using tools like Lang Chene, Open\n",
            "AI, and Google Gemini. He plans to continue this focus in 2024, with an emphasis on Gen.A.D.V.I and\n",
            "MLOPS. Additionally, he will also release a roadmap for becoming a data analyst in 2024. The roadmap\n",
            "is designed to be comprehensive, with multiple links, video types, and documentation types to cater\n",
            "to different learning styles. It also includes a variety of projects, with over 50 created to date.\n",
            "Nakhayak guarantees that the depth and breadth of his projects, including two with deployments, are\n",
            "unparalleled in other YouTube channels. His approach to project development involves a step-by-step\n",
            "process, starting with requirement gathering involving domain experts, product owners, and business\n",
            "analysts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import DeepLake\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "V4SYdWLkpL2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')"
      ],
      "metadata": {
        "id": "qiYBlgS9pL5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = f\"hub://ihamzakhan89/langchain_course_fewshot_selector\"\n",
        "db = DeepLake(dataset_path=dataset_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0Nf657an196",
        "outputId": "3a8adb81-a2c1-43c6-b1fd-225afb4d6e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Lake Dataset in hub://ihamzakhan89/langchain_course_fewshot_selector already exists, loading from the storage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
        "db.add_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO9SszTNpSAs",
        "outputId": "1a8e266d-cb04-4a5c-dc2f-2435a28765eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.vectorstores.deeplake:Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Lake Dataset in hub://ihamzakhan89/langchain_course_fewshot_selector already exists, loading from the storage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating 4 embeddings in 1 batches of size 4:: 100%|██████████| 1/1 [00:08<00:00,  8.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://ihamzakhan89/langchain_course_fewshot_selector', tensors=['embedding', 'id', 'metadata', 'text'])\n",
            "\n",
            "  tensor      htype       shape      dtype  compression\n",
            "  -------    -------     -------    -------  ------- \n",
            " embedding  embedding  (160, 1536)  float32   None   \n",
            "    id        text      (160, 1)      str     None   \n",
            " metadata     json      (160, 1)      str     None   \n",
            "   text       text      (160, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['43cd0766-b676-11ee-ae03-0242ac1c000c',\n",
              " '43cd0900-b676-11ee-ae03-0242ac1c000c',\n",
              " '43cd09a0-b676-11ee-ae03-0242ac1c000c',\n",
              " '43cd0a22-b676-11ee-ae03-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever()\n",
        "retriever.search_kwargs['distance_metric'] = 'cos'\n",
        "retriever.search_kwargs['k'] = 4"
      ],
      "metadata": {
        "id": "4ZLZnYajpSEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = \"\"\"Use the following pieces of transcripts from a video to answer the question in bullet points and summarized. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Summarized answer in bullter points:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "suNKiup_pcqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "qa = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                 chain_type=\"stuff\",\n",
        "                                 retriever=retriever,\n",
        "                                 chain_type_kwargs=chain_type_kwargs)\n",
        "\n",
        "print( qa.run(\"Explain lifecyle of Data Science project according to the video.\") )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUTjlCZUpl1Y",
        "outputId": "2ad4cd1b-da41-458a-8873-e3faeb37a296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- The lifecycle of a data science project begins with the work of a data scientist.\n",
            "- The first step is Requirement Gathering, where domain experts, product owners, and business analysts discuss and create a lot of stories. \n",
            "- The number of stories that need to be completed in each iteration or sprint is decided, and based on this, the program manager decides the team size.\n",
            "- All these requirements are then sent to the data team.\n",
            "- The lifecycle has remained the same for the past four to five years, but new tools have been introduced over time.\n",
            "- The video also mentions the use of MLOPS tools and the creation of multiple projects.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yUv1SGhmpcuT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}